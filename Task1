机器学习的一些概念
 a.监督学习(supervised learning)
   从给定的训练数据集中学习出一个函数（模型参数），当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入输出，
   也可以说是特征(feature)和目标(标签,label)。训练集中的目标是由人标注的。监督学习就是最常见的分类（注意和聚类区分）问题，通过已有的训练样本（即已知数据及其对应的输出）
   去训练得到一个最优模型（这个模型属于某个函数的集合，最优表示某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出，
   对输出进行简单的判断从而实现分类的目的。也就具有了对未知数据分类的能力。监督学习的目标往往是让计算机去学习我们已经创建好的分类系统（模型）。
   监督学习是训练神经网络和决策树的常见技术。这两种技术高度依赖事先确定的分类系统给出的信息，对于神经网络，分类系统利用信息判断网络的错误，
   然后不断调整网络参数。对于决策树，分类系统用它来判断哪些属性提供了最多的信息。

   常见的有监督学习算法：回归分析和统计分类。最典型的算法是KNN和SVM。

   有监督学习最常见的就是：regression&classification
   回归(Regression）
      回归问题是针对于连续型变量的。
      回归通俗一点就是，对已经存在的点（训练数据）进行分析，拟合出适当的函数模型y=f(x)，
      这里y就是数据的标签，而对于一个新的自变量x，通过这个函数模型得到标签y。 
   分类（Classification)  
      和回归最大的区别在于，分类是针对离散型的，输出的结果是有限的。结果必定是离散的，只有“是”或“否”。

 B.无监督学习
   输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，
   类间差距最大化。通俗点将就是实际应用中，不少情况下无法预先知道样本的标签，也就是说没有训练样本对应的类别，
   因而只能从原先没有样本标签的样本集开始学习分类器设计。

   无监督学习目标不是告诉计算机怎么做，而是让它（计算机）自己去学习怎样做事情。
   无监督学习有两种思路。第一种思路是在指导Agent时不为其指定明确分类，而是在成功时，采用某种形式的激励制度。
   需要注意的是，这类训练通常会置于决策问题的框架里，因为它的目标不是为了产生一个分类系统，而是做出最大回报的决定，
   这种思路很好的概括了现实世界，agent可以对正确的行为做出激励，而对错误行为做出惩罚。

   无监督学习的方法分为两大类：

   (1)一类为基于概率密度函数估计的直接方法：指设法找到各类别在特征空间的分布参数，再进行分类。

   (2)另一类是称为基于样本间相似性度量的简洁聚类方法：其原理是设法定出不同类别的核心或初始内核，
      然后依据样本与核心之间的相似性度量将样本聚集成不同的类别。
   利用聚类结果，可以提取数据集中隐藏信息，对未来数据进行分类和预测。应用于数据挖掘，模式识别，图像处理等。

    PCA和很多deep learning算法都属于无监督学习。 
   
  两者的不同点
  1.有监督学习方法必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律。而非监督学习没有训练集，只有一组数据，在该组数据集内寻找规律。
  2.有监督学习的方法就是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。而非监督学习方法只有要分析的数据集的本身，
    预先没有什么标签。如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不予以某种预先分类标签对上号为目的。
  3.非监督学习方法在寻找数据集中的规律性，这种规律性并不一定要达到划分数据集的目的，也就是说不一定要“分类”。

   这一点是比有监督学习方法的用途要广。    譬如分析一堆数据的主分量，或分析数据集有什么特点都可以归于非监督学习方法的范畴。

  4.用非监督学习方法分析数据集的主分量与用K-L变换计算数据集的主分量又有区别。后者从方法上讲不是学习方法。
    因此用K-L变换找主分量不属于无监督学习方法，即方法上不是。而通过学习逐渐找到规律性这体现了学习方法这一点。
    在人工神经元网络中寻找主分量的方法属于无监督学习方法。 
 
 何时采用哪种方法
 
 简单的方法就是从定义入手，有训练样本则考虑采用监督学习方法；无训练样本，则一定不能用监督学习方法。
 但是，现实问题中，即使没有训练样本，我们也能够凭借自己的双眼，从待分类的数据中，人工标注一些样本，并把它们作为训练样本，
 这样的话，可以把条件改善，用监督学习方法来做。
 对于不同的场景，正负样本的分布如果会存在偏移（可能大的偏移，可能比较小），这样的话，监督学习的效果可能就不如用非监督学习了。
 
 *想法:
      在现实的生产环境中,往往都是在无监督学习开始的,因为很多业务都是从无到有,所以一开始一般都是通过收集一下相关的数据进行分析挖掘,
      所以没有明确的训练集和测试集,所以只能从无监督学习开始,当业务进行到一个阶段以后,开始收集到相关的业务数据,做成训练集和测试集,
      可以有无监督学习尝试转到监督学习,最后两者校对,选择最优的学习方法.
 
 
 C.泛化能力
   在机器学习方法中，泛化能力通俗来讲就是指学习到的模型对未知数据的预测能力。在实际情况中，我们通常通过测试误差来评价学习方法的泛化能力。
   如果在不考虑数据量不足的情况下出现模型的泛化能力差，那么其原因基本为对损失函数的优化没有达到全局最优。
   #泛化误差
    根据PAC理论，泛化误差可以直观理解为以e指数的形式正比于假设空间的复杂度，反比于数据量的个数。就是数据量越多，模型效果越好，
    模型假设空间复杂度越简单，模型效果越好。
   #提高泛化能力
    提高泛化能力的方式大致有三种：1.增加数据量。2.正则化。3.凸优化。
   
 D.过拟合以及欠拟合(方差和偏差以及各自解决办法)
   #过拟合
    根本原因是特征维度过多，模型假设过于复杂，参数过多，训练数据过少，噪声过多，导致拟合的函数完美的预测训练集，但对新数据的测试集预测结果差。 
    过度的拟合了训练数据，而没有考虑到泛化能力。
　　
    解决方法：（1）减少特征维度；（2）正则化，降低参数值。
   #欠拟合
    根本原因是特征维度过少，模型过于简单，导致拟合的函数无法满足训练集，误差较大； 
　　解决方法：增加特征维度，增加训练数据。
   
   减少过拟合总结：过拟合主要是有两个原因造成的：数据太少+模型太复杂 
　　（1）获取更多数据 ：从数据源头获取更多数据；数据增强（Data Augmentation） 
　　（2）使用合适的模型：减少网络的层数、神经元个数等均可以限制网络的拟合能力； 
　　（3）dropout ；
　　（4）正则化，在训练的时候限制权值变大； 
　　（5）限制训练时间；通过评估测试； 
　　（6）增加噪声 Noise： 输入时+权重上（高斯初始化） ；
    （7）数据清洗(data ckeaning/Pruning)：将错误的label 纠正或者删除错误的数据。
    （8）结合多种模型： Bagging用不同的模型拟合不同部分的训练集；Boosting只使用简单的神经网络；
   
   产生过拟合根本原因：

　　1、 观察值与真实值存在偏差： 
　　 训练样本的获取，本身就是一种 抽样。抽样操作就会存在误差， 也就是你的训练样本 取值 X， X = x(真值) + u（随机误差)，机器学习的 优化函数 多为 min Cost函数，自然就是尽可能的拟合 X，而不是真实的x,所以 就称为过拟合了，实际上是学习到了真实规律以外的 随机误差。举个例子说，你想做人脸识别，人脸里有背景吧，要是你这批人脸背景A都相似，学出来的模型，见到背景A，就会认为是人脸。这个背景A就是你样本引入的误差。
　　2、 数据太少，导致无法描述问题的真实分布
 　　举个例子，投硬币问题 是一个 二项分布，但是如果 你碰巧投了10次，都是正面。那么你根据这个数据学习，是无法揭示这个规律的，根据统计学的大数定律（通俗地说，这个定理就是，在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率），当样本多了，这个真实规律是必然出现的。
　　为什么说 数据量大了以后 就能防止过拟合，数据量大了，
    问题2，不再存在，
    问题1，在求解的时候因为数据量大了， 求解min Cost函数时候， 模型为了求解到最小值过程中，需要兼顾真实数据拟合 和 随机误差拟合，所有样本的真实分布是相同的（都是人脸），而随机误差会一定程度上抵消（背景），

　　（1）数据有噪声。

　　我们可以理解地简单些：有噪音时，更复杂的模型会尽量去覆盖噪音点，即对数据过拟合。这样，即使训练误差Ein 很小（接近于零），由于没有描绘真实的数据趋势，Eout 反而会更大。
　　即噪音严重误导了我们的假设。还有一种情况，如果数据是由我们不知道的某个非常非常复杂的模型产生的，实际上有限的数据很难去“代表”这个复杂模型曲线。我们采用不恰当的假设去尽量拟合这些数据，效果一样会很差，因为部分数据对于我们不恰当的复杂假设就像是“噪音”，误导我们进行过拟合。
    （2）训练数据不足，有限的训练数据。

　　（3）训练模型过度，导致模型非常复杂。
  
  2、正则方法主要有哪些？

（1）L1和L2正则：都是针对模型中参数过大的问题引入惩罚项，依据是奥克姆剃刀原理。在深度学习中，L1会趋向于产生少量的特征，而其他的特征都是0增加网络稀疏性；而L2会选择更多的特征，这些特征都会接近于0，防止过拟合。神经网络需要每一层的神经元尽可能的提取出有意义的特征，而这些特征不能是无源之水，因此L2正则用的多一些。

（2）dropout：深度学习中最常用的正则化技术是dropout，随机的丢掉一些神经元。

（3）数据增强，比如将原始图像翻转平移拉伸，从而是模型的训练数据集增大。数据增强已经是深度学习的必需步骤了，其对于模型的泛化能力增加普遍有效，但是不必做的太过，将原始数据量通过数据增加增加到2倍可以，但增加十倍百倍就只是增加了训练所需的时间，不会继续增加模型的泛化能力了。

（4）提前停止（early stopping）：就是让模型在训练的差不多的时候就停下来，比如继续训练带来提升不大或者连续几轮训练都不带来提升的时候，这样可以避免只是改进了训练集的指标但降低了测试集的指标。

（5）批量正则化（BN）：就是将卷积神经网络的每层之间加上将神经元的权重调成标准正态分布的正则化层，这样可以让每一层的训练都从相似的起点出发，而对权重进行拉伸，等价于对特征进行拉伸，在输入层等价于数据增强。注意正则化层是不需要训练。
 
 E.交叉验证
   以下简称交叉验证(Cross Validation)为CV.CV是用来验证分类器的性能一种统计分析方法,基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set),首先用训练集对分类器进行训练,在利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标.常见CV的方法如下:

  1).Hold-Out Method
     将原始数据随机分为两组,一组做为训练集,一组做为验证集,利用训练集训练分类器,然后利用验证集验证模型,记录最后的分类准确率为此Hold-OutMethod下分类器的性能指标.此种方法的好处的处理简单,只需随机把原始数据分为两组即可,其实严格意义来说Hold-Out Method并不能算是CV,因为这种方法没有达到交叉的思想,由于是随机的将原始数据分组,所以最后验证集分类准确率的高低与原始数据的分组有很大的关系,所以这种方法得到的结果其实并不具有说服性.


   2).K-fold Cross Validation(记为K-CV)K折交叉验证
      将原始数据分成K组(一般是均分),将每个子集数据分别做一次验证集,其余的K-1组子集数据作为训练集,这样会得到K个模型,用这K个模型最终的验证集的分类准确率的平均数作为此K-CV下分类器的性能指标.K一般大于等于2,实际操作时一般从3开始取,只有在原始数据集合数据量小的时候才会尝试取2.K-CV可以有效的避免过学习以及欠学习状态的发生,最后得到的结果也比较具有说服性.

   3).Leave-One-Out Cross Validation(记为LOO-CV)留一验证
      如果设原始数据有N个样本,那么LOO-CV就是N-CV,即每个样本单独作为验证集,其余的N-1个样本作为训练集,所以LOO-CV会得到N个模型,用这N个模型最终的验证集的分类准确率的平均数作为此下LOO-CV分类器的性能指标.相比于前面的K-CV,LOO-CV有两个明显的优点:

   a.每一回合中几乎所有的样本皆用于训练模型,因此最接近原始样本的分布,这样评估所得的结果比较可靠。
   b.实验过程中没有随机因素会影响实验数据,确保实验过程是可以被复制的。
     但LOO-CV的缺点则是计算成本高,因为需要建立的模型数量与原始数据样本数量相同,当原始数据样本数量相当多时,LOO-CV在实作上便有困难几乎就是不显示,除非每次训练分类器得到模型的速度很快,或是可以用并行化计算减少计算所需的时间. 

   4) Jackknife(刀切法)
      是有Maurice Quenouille (1949)提出的一种再抽样方法，其原始动机是降低估计的偏差, 作为一种通用的假设检验和置信区间计算的方法。

      Jackknife类似于“Leave one out”的交叉验证方法。令X=(X1,X2,…,Xn)为观测到的样本，定义第i个Jackknife样本为丢掉第i个样本后的剩余样本即

      由此生成的Jackknife样本集之间的差异很小，每两个Jackknife样本中只有两个单个的原始样本不同。

      Jackknife不适合的场合
         统计函数不是平滑函数：数据小的变化会带来统计量的一个大的变化如极值、中值。如对数据X=(10,27,31,40,46,50,52,104,146)的中值得到的结果为48,48,48,48,45,43,43,43,43,偶数个数的中值为最中间两个数的平均值。

    Jackknife与Bootstrap自助法的联系
        Efron1979年文章指出了自助法与刀切法的关系。首先，自助法通过经验分布函数构建了自助法世界，将不适定的估计概率分布的问题转化为从给定样本集中重采样。第二，自助法可以解决不光滑参数的问题。遇到不光滑(Smooth)参数估计时，刀切法会失效，而自助法可以有效地给出中位数的估计。第三，将自助法估计用泰勒公式展开，可以得到刀切法是自助法方法的一阶近似。第四，对于线性统计量的估计方差这个问题，刀切法或者自助法会得到同样的结果。但在非线性统计量的方差估计问题上，刀切法严重依赖于统计量线性的拟合程度，所以远不如自助法有效。
        线性回归的原理

 F.线性回归
   线性回归假设特征和结果满⾜线性关系。其实线性关系的表达能⼒⾮常强⼤，每个特征对结果的影响强弱可以由前⾯的参数体现，⽽且每个特征变量可以⾸先映射到⼀个函数，然后再参与线性
   计算。这样就可以表达特征与结果之间的⾮线性关系。
  a.损失函数
    损失函数分为经验风险损失函数和结构风险损失函数，经验风险损失函数反映的是预测结果和实际结果之间的差别，结构风险损失函数则是经验风险损失函数加上正则项(L0、L1（Lasso）、L2（Ridge）)。

 G.线性回归的评估指标
   在分类模型中针对不同的数据我们可以用分类的准确度评价谁的模型效果较好，这两者的量纲是一致的，但是在回归中预测不同的实际场景，比如一个预测股市，一个预测房价，比较MSE或者RMSE就不能比较谁好谁坏；所以将预测结果转换为准确度，结果都在[0, 1]之间，针对不同问题的预测准确度，可以比较并来判断此模型更适合预测哪个问题

 H.sklearn参数详解
      a.KNN

　　        •n_neighbors：默认为5，就是k-NN的k的值，选取最近的k个点。

　　        •weights：默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，就说所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。

　　        •algorithm：快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。balltree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。

　　        •leaf_size：默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。

　　        •metric：用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。

　　        •p：距离度量公式。在上小结，我们使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。

　　        •metric_params：距离公式的其他关键参数，这个可以不管，使用默认的None即可。

　　        •n_jobs：并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。

　　    b.Kmeans

　　        •n_clusters:簇的个数，即你想聚成几类

　　        •init:初始簇中心的获取方法

　　        •n_init:获取初始簇中心的更迭次数，为了弥补初始质心的影响，算法默认会初始10次质心，实现算法，然后返回最好的结果。

　　        •max_iter:最大迭代次数（因为kmeans算法的实现需要迭代）

　　        •tol:容忍度，即kmeans运行准则收敛的条件

　　        •precompute_distances：是否需要提前计算距离，这个参数会在空间和时间之间做权衡，如果是True会把整个距离矩阵都放到内存中，auto会默认在数据样本大于featurs*samples的数量大于12e6的时候False,False时核心实现的方法是利用Cpython来实现的

　　        •verbose:冗长模式（不太懂是啥意思，反正一般不去改默认值）

　　        •random_state:随机生成簇中心的状态条件。

　　        •copy_x:对是否修改数据的一个标记，如果True，即复制了就不会修改数据。bool在scikit-learn很多接口中都会有这个参数的，就是是否对输入数据继续copy操作，以便不修改用户的输入数据。这个要理解Python的内存机制才会比较清楚。

　　        •n_jobs:并行设置

　　        •algorithm:kmeans的实现算法，有：’auto’,‘full’,‘elkan’,其中‘full’表示用EM方式实现

　　    c.朴素贝叶斯

　　        i.高斯朴素贝叶斯：

　　        •priors:先验概率大小，如果没有给定，模型则根据样本数据自己计算（利用极大似然法）。

　　        对象

　　        •class_prior_:每个样本的概率

　　        •class_count:每个类别的样本数量

　　        •theta_:每个类别中每个特征的均值

　　        •sigma_:每个类别中每个特征的方差

　　        ii.伯努利朴素贝叶斯

　　        •alpha:平滑因子，与多项式中的alpha一致。

　　        •binarize:样本特征二值化的阈值，默认是0。如果不输入，则模型会认为所有特征都已经是二值化形式了；如果输入具体的值，则模型会把大于该值的部分归为一类，小于的归为另一类。

　　        •fit_prior:是否去学习类的先验概率，默认是True

　　        •class_prior:各个类别的先验概率，如果没有指定，则模型会根据数据自动学习，每个类别的先验概率相同，等于类标记总个数N分之一。

　　    d.决策树：

　　        •criterion:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。

　　        •splitter:特征切分点选择标准，决策树是递归地选择最优切分点，spliter是用来指明在哪个集合上来递归，有“best”和“random”两种参数可以选择，best表示在所有特征上递归，适用于数据集较小的时候，random表示随机选择一部分特征进行递归，适用于数据集较大的时候。

　　        •max_depth:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。

　　        •min_samples_split:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。

　　        •min_samples_leaf:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。

　　        •min_weight_fraction_leaf:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。

　　        •max_features:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。

　　        •random_state:随机种子的设置，与LR中参数一致。

　　        •max_leaf_nodes:最大叶节点个数，即数据集切分成子数据集的最大个数。

　　        •min_impurity_decrease:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。

　　        •min_impurity_split:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。

　　        •class_weight:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。

　　        •presort:是否进行预排序，默认是False，所谓预排序就是提前对特征进行排序，我们知道，决策树分割数据集的依据是，优先按照信息增益/基尼系数大的特征来进行分割的，涉及的大小就需要比较，如果不进行预排序，则会在每次分割的时候需要重新把所有特征进行计算比较一次，如果进行了预排序以后，则每次分割的时候，只需要拿排名靠前的特征就可以了。

　　        对象/属性

　　        §classes_:分类模型的类别，以字典的形式输出

　　        §feature_importances_:特征重要性，以列表的形式输出每个特征的重要性max_features_:最大特征数

　　        §n_classes_:类别数，与classes_对应，classes_输出具体的类别

　　        §n_features_:特征数，当数据量小时，一般max_features和n_features_相等

　　        §n_outputs_:输出结果数tree_:输出整个决策树,用于生成决策树的可视化

　　        方法

　　        §decision_path(X):返回X的决策路径fit(X,y):在数据集(X,y)上使用决策树模型

　　        §get_params([deep]):获取模型的参数

　　        §predict(X):预测数据值X的标签

　　        §predict_log_proba(X):返回每个类别的概率值的对数

　　        §predict_proba(X):返回每个类别的概率值（有几类就返回几列值）

　　score(X,y):返回给定测试集和对应标签的平均准确率
   
