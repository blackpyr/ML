1、逻辑回归与线性回归的联系与区别 
  逻辑回归与线性回归都属于广义线性回归模型,其区别与联系从以下几个方面比较：
  分类与回归:回归模型就是预测一个连续变量(如降水量，价格等)。在分类问题中，预测属于某类的概率，可以看成回归问题。这可以说是使用回归算法的分类方法。
  输出:直接使用线性回归的输出作为概率是有问题的，因为其值有可能小于0或者大于1,这是不符合实际情况的，逻辑回归的输出正是[0,1]区间。
  参数估计方法：

  线性回归中使用的是最小化平方误差损失函数，对偏离真实值越远的数据惩罚越严重。这样做会有什么问题呢？假如使用线性回归对{0,1}二分类问题做预测，则一个真值为1的样本，其预测值为50，那么将会对其产生很大的惩罚，这也和实际情况不符合，更大的预测值说明为1的可能性越大，而不应该惩罚的越严重。
  逻辑回归使用对数似然函数进行参数估计，使用交叉熵作为损失函数，对预测错误的惩罚是随着输出的增大，逐渐逼近一个常数，这就不存在上述问题了1
  也正是因为使用的参数估计的方法不同，线性回归模型更容易受到异常值(outlier)的影响，有可能需要不断变换阈值(threshold).
  
  参数解释: 
          线性回归中，独立变量的系数解释十分明了，就是保持其他变量不变时，改变单个变量因变量的改变量。
          逻辑回归中，自变量系数的解释就要视情况而定了，要看选用的概率分布是什么，如二项式分布，泊松分布等
2、 逻辑回归的原理 
  逻辑回归用于分类问题。在分类问题中，我们尝试预测目前观测目标属于哪一类，它会产生一个离散的二元结果y∈{0,1}。而线性回归模型产生的预测值为z=θTx是实数值，于是我们引入一个新的模型，使输出变量z的值到始终在0和1之间，于是便找到了Sigmoid function。


3、逻辑回归损失函数推导及优化 

4、 正则化与模型评估指标 

5、逻辑回归的优缺点 

6、样本不均衡问题解决办法 

7. sklearn参数
